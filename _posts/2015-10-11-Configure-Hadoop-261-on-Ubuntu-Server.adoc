= Configure Hadoop 2.6.1 on Ubuntu Server
:hp-tags: Hadoop, Configuration

#### SSH setup and Key generate
*SSH* setup is required to do different operations on a cluster such as starting, stopping, distributed daemon shell operations. To authenticate different users of Hadoop, it is required to provide public/private key pair for a Hadoop user and share it with different users.

The following commands are used for generating a key value pair using SSH. Copy the public keys form id_rsa.pub to authorized_keys, and provide the owner with read and write permissions to authorized_keys file respectively.
```
$ ssh-keygen -t rsa 
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys 
$ chmod 0600 ~/.ssh/authorized_keys 
```

#### Install Java
pass

Must set JAVA_HOME

#### Download Hadoop
```
# cd /usr/local 
# sudo wget http://www.us.apache.org/dist/hadoop/common/hadoop-2.6.1/hadoop-2.6.1.tar.gz 
# sudo tar -txzf hadoop-2.6.1.tar.gz
```
#### Hadoop Operation Modes
Once you have downloaded Hadoop, you can operate your Hadoop cluster in one of the three supported modes:

* *Standalone Mode* : After downloading Hadoop in your system, *by default*, it is configured in a standalone mode and can be run as a single java process.

* *Pseudo Distributed Mode* : It is a distributed simulation on single machine. Each Hadoop daemon such as hdfs, yarn, MapReduce etc., will run as a separate java process. This mode is useful for development.

* *Fully Distributed Mode* : This mode is fully distributed with minimum two or more machines as a cluster. We will come across this mode in detail in the coming chapters.

#### Installing Hadoop in Standalone Mode

There are no daemons running and everything runs in a single JVM. Standalone mode is suitable for running *MapReduce programs during development*, since it is easy to test and debug them.

* Setting Up Hadoop
You can set Hadoop environment variables by appending the following commands to ~/.*bashrc* file.
```
export HADOOP_HOME=/usr/local/hadoop 
```

Verify your configuration by:
```
hadoop version
```

##### Configure environment variable
```
export HADOOP_HOME=/usr/local/hadoop-2.6.1
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
export HADOOP_INSTALL=$HADOOP_HOME
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

Apply all changes by:
```
source .bashrc
```

#### Configure Pseudo Distributed Mode
All hadoop configuration files are in the file *$HADOOP_HOME/etc/hadoop*, It is required to make changes in those configuration files according to your Hadoop infrastructure.

* *hadoop-env.sh*

In order to *develop Hadoop programs in java*, you have to reset the java environment variables in *hadoop-env.sh* file by replacing JAVA_HOME value with the location of java in your system.
```
export JAVA_HOME=YOUR_JAVA_DIRECTORY
```

* *core-site.xml*

The *core-site.xml* file contains information such as the port number used for Hadoop instance, memory allocated for the file system, memory limit for storing the data, and size of Read/Write buffers.

Open the core-site.xml and add the following properties in between <configuration>, </configuration> tags.

```
<configuration>
  <property>
    <name>fs.default.name</name>
    <value>hdfs://localhost:8020</value>
  </property>
</configuration>
```

* *hdfs-site.xml*

The *hdfs-site.xml* file contains information such as the value of replication data, *namenode path*, and *datanode paths* of your local file systems. It means the place where you want to store the Hadoop infrastructure.
```
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.name.dir</name>
    <value>file:///home/jing/hadoop/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.data.dir</name>
    <value>file:///home/jing/hadoop/hdfs/datanode</value>
  </property>
</configuration>
```

* *yarn-site.xml*

This file is used to configure yarn into Hadoop.
```
<configuration>
 
   <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value> 
   </property>
  
</configuration>
```

* *mapred-core.xml*

This file is used to specify which MapReduce framework we are using.

```
<configuration>
 
   <property> 
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
   </property>
   
</configuration>
```


#### Namenode setup
```
hdfs namenode -format 
```

start dfs by:
```
start-dfs.sh
```

start yarn by:
```
start-yarn.sh
```

#### Status

Access status of namenode by :

http://localhost:50070/

Aceess status of datanode by:

http://localhost:50075/


#### Configuration files

Check my configuration files at

https://github.com/milantracy/hadoop-configuration